import requests
import re
import os
import sys
from datetime import datetime
from urllib.parse import quote
import hashlib

def get_base_dir():
    """è·å–ä»“åº“æ ¹ç›®å½•"""
    if 'GITHUB_WORKSPACE' in os.environ:
        return os.environ['GITHUB_WORKSPACE']
    return os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

def extract_rule_domain(rule):
    """ä»è§„åˆ™ä¸­æå–åŸŸåéƒ¨åˆ†"""
    match = re.match(r'^(@@\|\||\|\|)([^\^$]+)', rule)
    return match.group(2).rstrip('/') if match else None

def process_lite_rules(strict_rules):
    """ç”Ÿæˆ OAdH_NCR è§„åˆ™å¹¶è¿”å›(è§„åˆ™åˆ—è¡¨, é‡å¤æ•°)"""
    whitelist = []
    whitelist_domains = set()
    blacklist = []
    duplicates = 0

    for rule in strict_rules:
        rule = rule.strip()
        if not rule:
            continue
        
        if rule.startswith('@@||'):
            domain = extract_rule_domain(rule)
            if domain:
                whitelist_domains.add(domain)
                whitelist.append(rule)
        elif rule.startswith('||'):
            blacklist.append(rule)

    # è¿‡æ»¤é»‘åå•å¹¶è®¡æ•°
    filtered_blacklist = []
    for rule in blacklist:
        domain = extract_rule_domain(rule)
        if domain in whitelist_domains:
            duplicates += 1
        else:
            filtered_blacklist.append(rule)

    return (filtered_blacklist + whitelist, duplicates)

def process_urls(urls):
    """å¤„ç† URL æºæ•°æ®"""
    results = {
        'normal': [],
        'strict': [],
        'sources': [],
        'stats': {
            'total_urls': len(urls),
            'total_lines': 0,
            'normal': {'valid': 0, 'duplicates': 0},
            'strict': {'valid': 0, 'duplicates': 0}
        }
    }

    for url in urls:
        try:
            response = requests.get(url, timeout=15)
            response.raise_for_status()
            
            normal_count = 0
            strict_count = 0
            normal_lines = []
            strict_lines = []
            
            for line in response.text.splitlines():
                stripped = line.strip()
                results['stats']['total_lines'] += 1
                
                if not stripped or stripped.startswith('!'):
                    continue
                
                normal_count += 1
                normal_lines.append(line)
                
                if stripped.startswith(('||', '@@')):
                    strict_count += 1
                    strict_lines.append(line)
            
            results['sources'].append({
                'url': url,
                'normal': normal_count,
                'strict': strict_count
            })
            
            results['normal'].extend(normal_lines)
            results['strict'].extend(strict_lines)
            
        except Exception as e:
            print(f"å¤„ç†å¤±è´¥ï¼š{str(e)}")
            results['sources'].append({
                'url': url,
                'normal': 0,
                'strict': 0
            })
            continue

    # å»é‡å¤„ç†
    for mode in ['normal', 'strict']:
        seen = set()
        unique = []
        duplicates = 0
        for line in results[mode]:
            if line not in seen:
                seen.add(line)
                unique.append(line)
            else:
                duplicates += 1
        results[mode] = unique
        results['stats'][mode]['valid'] = len(unique)
        results['stats'][mode]['duplicates'] = duplicates
    
    return results

def safe_write_file(path, content):
    """å®‰å…¨å†™å…¥æ–‡ä»¶ï¼Œä»…åœ¨å†…å®¹å˜åŒ–æ—¶æ›´æ–°ï¼Œè¿”å›æ˜¯å¦å‘ç”Ÿå˜æ›´"""
    # ç»Ÿä¸€æ¢è¡Œç¬¦ä¸ºLF
    normalized_content = '\n'.join(content.splitlines()) + '\n'
    
    # æ£€æµ‹æ˜¯å¦éœ€è¦å†™å…¥
    if os.path.exists(path):
        with open(path, 'r', encoding='utf-8') as f:
            existing_content = f.read()
        if existing_content == normalized_content:
            return False
    
    # å†™å…¥æ–‡ä»¶
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        f.write(normalized_content)
    return True

README_TEMPLATE = """# ğŸ›¡ï¸ AdGuard Home è§„åˆ™åº“
---
## ğŸ¤” ç®€ä»‹
èåˆå¤šä¸ªAdguardè§„åˆ™ï¼Œæœ€ç»ˆç­›é€‰å‡ºé€‚ç”¨äºAdguard Homeä½¿ç”¨çš„DNSè§„åˆ™  
- å…¨éƒ¨è§„åˆ™ï¼šä»…å»é‡  
- OAdHè§„åˆ™(OAdH_ALL)ï¼šä»…DNSè§„åˆ™ **(æ¨è)**
- OAdHå»å†²çªè§„åˆ™(OAdH_NCR)ï¼šå»é™¤é»‘ç™½åå•åŒæ—¶å­˜åœ¨çš„æƒ…å†µ **(å®éªŒæ€§)**
---

## ğŸ“¦ å½“å‰ç‰ˆæœ¬
**ç‰ˆæœ¬æ ‡è¯†**: {version}  
**æ›´æ–°æ—¶é—´**: {timestamp}  

---

## ğŸ“‚ æ•°æ®æºåˆ—è¡¨
{source_section}

---

## ğŸ“Š è§„åˆ™ç»Ÿè®¡
{stats_section}

---

## ğŸ“¥ æ–‡ä»¶ä¸‹è½½
### ğŸŒç›´è¿
{download_links}
### ğŸš€åŠ é€Ÿ
{download_links_cn}
---
"""

def update_readme(stats, sources, lite_info):
    # ç”Ÿæˆç‰ˆæœ¬ä¿¡æ¯
    version = f"v{datetime.utcnow().strftime('%Y%m%d%H%M')}"
    
    # ç”Ÿæˆå¸¦æ ·å¼çš„æ•°æ®æºè¡¨æ ¼
    def format_source(source):
        encoded_url = quote(source['url'], safe='/:')
        return f"| ğŸ”— [{source['url']}]({encoded_url}) | `{source['normal']}` | `{source['strict']}` |"
    
    source_table = [
        "| æ•°æ®æºåœ°å€ | æºè§„åˆ™æ•° | OAdHè§„åˆ™æ•° |",
        "|----------|-----------|-----------|",
        *map(format_source, sources)
    ]
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯å¡ç‰‡
    stats_cards = [
        "**å…¨éƒ¨è§„åˆ™**ï¼š",
        f"- æœ‰æ•ˆè§„åˆ™ï¼š`{stats['normal']['valid']}`  "  # æœ«å°¾åŒç©ºæ ¼å¼ºåˆ¶æ¢è¡Œ
        f"- é‡å¤è¿‡æ»¤ï¼š`{stats['normal']['duplicates']}`\n",
        
        "**OAdHè§„åˆ™ (OAdH_ALL)**ï¼š",
        f"- æœ‰æ•ˆè§„åˆ™ï¼š`{stats['strict']['valid']}`  "
        f"- é‡å¤è¿‡æ»¤ï¼š`{stats['strict']['duplicates']}`\n",
        
        "**OAdHå»å†²çªè§„åˆ™ (OAdH_NCR)**ï¼š",
        f"- æœ‰æ•ˆè§„åˆ™ï¼š`{len(lite_info[0])}`  "
        f"- å†²çªè¿‡æ»¤ï¼š`{lite_info[1]}`"
    ]
    
    # ç”Ÿæˆå¸¦å›¾æ ‡çš„ä¸‹è½½é“¾æ¥
    download_links = [
        "ğŸ”— [å…¨éƒ¨è§„åˆ™ (all.txt)](dist/all.txt)  ",
        "ğŸ”’ [OAdHè§„åˆ™ (OAdH_ALL.txt)](dist/OAdH_ALL.txt)  ",
        "âœ‚ï¸ [OAdHå»å†²çªè§„åˆ™ (OAdH_NCR.txt)](dist/OAdH_NCR.txt)"
    ]
    download_links_cn = [
        "ğŸ”— [å…¨éƒ¨è§„åˆ™ (all.txt)](https://github.snakexgc.com/https://github.com/snakexgc/OnlyAdguardHomeRules/blob/main/dist/all.txt)  ",
        "ğŸ”’ [OAdHè§„åˆ™ (OAdH_ALL.txt)](https://github.snakexgc.com/https://github.com/snakexgc/OnlyAdguardHomeRules/blob/main/dist/OAdH_ALL.txt)  ",
        "âœ‚ï¸ [OAdHå»å†²çªè§„åˆ™ (OAdH_NCR.txt)](https://github.snakexgc.com/https://github.com/snakexgc/OnlyAdguardHomeRules/blob/main/dist/OAdH_NCR.txt)"
    ]
    
    # ç»„è£…å†…å®¹
    content = README_TEMPLATE.format(
        version=version,
        timestamp=datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC'),
        source_section="\n".join(source_table),
        stats_section="\n".join(stats_cards),
        download_links="\n".join(download_links),
        download_links_cn="\n".join(download_links_cn)
    )
    
    # å˜æ›´æ£€æµ‹ï¼ˆå“ˆå¸Œæ ¡éªŒï¼‰
    readme_path = os.path.join(get_base_dir(), 'README.md')
    current_hash = hashlib.md5(content.encode()).hexdigest()
    
    if os.path.exists(readme_path):
        with open(readme_path, 'rb') as f:
            if hashlib.md5(f.read()).hexdigest() == current_hash:
                return False
    
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(content)
    return True

if __name__ == '__main__':
    BASE_DIR = get_base_dir()
    has_changes = False
    
    # è¯»å–æºæ–‡ä»¶
    source_path = os.path.join(BASE_DIR, 'source.txt')
    try:
        with open(source_path, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")
        sys.exit(1)

    results = process_urls(urls)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.join(BASE_DIR, 'dist')
    os.makedirs(output_dir, exist_ok=True)

    # å†™å…¥åŸºç¡€è§„åˆ™æ–‡ä»¶
    has_changes |= safe_write_file(
        os.path.join(output_dir, 'all.txt'),
        "\n".join(results['normal'])
    )
    has_changes |= safe_write_file(
        os.path.join(output_dir, 'OAdH_ALL.txt'),
        "\n".join(results['strict'])
    )

    # ç”Ÿæˆ OAdH_NCR è§„åˆ™
    with open(os.path.join(output_dir, 'OAdH_ALL.txt'), 'r', encoding='utf-8') as f:
        oadh_all_rules = f.read().splitlines()
    
    oadh_ncr_rules, oadh_ncr_duplicates = process_lite_rules(oadh_all_rules)
    has_changes |= safe_write_file(
        os.path.join(output_dir, 'OAdH_NCR.txt'),
        "\n".join(oadh_ncr_rules)
    )

    # æ›´æ–°README
    readme_changed = update_readme(results['stats'], results['sources'], (oadh_ncr_rules, oadh_ncr_duplicates))
    has_changes |= readme_changed

    # è®¾ç½®è¾“å‡ºå˜é‡
    with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
        print(f'has_changes={str(has_changes).lower()}', file=fh)
    
    print(f"å¤„ç†å®Œæˆ{'ï¼Œæ£€æµ‹åˆ°å˜æ›´' if has_changes else 'ï¼Œæ— æ–°å˜æ›´'}")
